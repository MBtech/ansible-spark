---
# tasks file for create_spark_cluster
- include: security_groups.yml

# - name: get the ubuntu trusty AMI
#   ec2_ami_search: distro=ubuntu release=bionic virt=hvm region={{ region }} store=ebs-ssd
#   register: ubuntu_image
#   tags: always

- name: start the spark master
  ec2:
    image: "{{ ubuntu_image_ami }}"
    region: "{{ region }}"
    instance_type: "{{ master_instance_type }}"
    profile: "{{ boto_profile }}"
    key_name: "{{ ec2_key_pair }}"
    group: [web, ssh, outbound, spark, hadoop]
    instance_tags: { Name: spark_master, ds_tech: spark, ds_role: spark_master }
    exact_count: 1
    count_tag: { ds_role: spark_master }
    instance_profile_name: "{{ instance_profile_arn }}"
    volumes:
      - device_name: /dev/sda1
        volume_type: gp2
        volume_size: "{{ ec2_master_volume_size }}"
        delete_on_termination: true
    wait: yes
  register: spark_master
  tags: always

- name: start the spark slaves on spot instances
  ec2:
    image: "{{ ubuntu_image_ami }}"
    region: "{{ region }}"
    instance_type: "{{ instance_type }}"
    profile: "{{ boto_profile }}"
    key_name: "{{ ec2_key_pair }}"
    group: [web, ssh, outbound, spark, hadoop]
    instance_tags: { Name: spark_slave, ds_tech: spark, ds_role: spark_slave }
    exact_count: "{{ slave_count }}"
    count_tag: { ds_role: spark_slave }
    spot_price: "{{ spot_price }}"
    instance_initiated_shutdown_behavior: "terminate"
    instance_profile_name: "{{ instance_profile_arn }}"
    volumes:
      - device_name: /dev/sda1
        volume_type: gp2
        volume_size: "{{ ec2_volume_size }}"
        delete_on_termination: true
    wait: yes
  register: spark_slave
  tags: always
  when: spot_price is defined

- name: Display spark master info
  debug:
    msg: "{{ spark_master }}"

- name: Display spark Slave info
  debug:
    msg: "{{ spark_slave }}"

- name: wait for ssh server to be running
  wait_for: host={{ item.public_dns_name }} port=22 search_regex=OpenSSH
  when: item.public_dns_name is defined
  with_items: "{{ spark_master.tagged_instances }}"
  tags: always

- name: wait for ssh server to be running
  wait_for: host={{ item.public_dns_name }} port=22 search_regex=OpenSSH
  when: item.public_dns_name is defined
  with_items: "{{ spark_slave.tagged_instances }}"
  tags: always

- name: refresh ec2 cache
  command: /etc/ansible/ec2.py --refresh-cache

- name: Refresh in-memory EC2 cache
  meta: refresh_inventory
